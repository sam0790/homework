{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "educational-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.combine import SMOTEEN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-proxy",
   "metadata": {},
   "source": [
    "### 1.\tPerform combined over and undersampling on the diabetes dataset (use SMOTEENN). Explain how combined sampling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "exotic-crown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia_df = pd.read_csv(\"../diabetes.csv\")\n",
    "dia_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "flush-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dia_df.drop('Outcome', axis=1)\n",
    "y = dia_df['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45,stratify=y)\n",
    "\n",
    "sc= StandardScaler()\n",
    "\n",
    "X_sc_train=sc.fit_transform(X_train)\n",
    "X_sc_test=sc.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "concerned-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object\n",
    "smote = SMOTEENN(random_state=45)\n",
    "    \n",
    "# Fit the data\n",
    "X_resampled,y_resampled = smote.fit_resample(X_sc_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "original-anaheim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN performance score : 0.7801851851851851\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=45)\n",
    "\n",
    "lr.fit(X_resampled,y_resampled)\n",
    "\n",
    "y_pred = lr.predict(X_sc_test)\n",
    "\n",
    "acc_score = balanced_accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"SMOTEENN performance score :\",acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-oracle",
   "metadata": {},
   "source": [
    "### 2.\tComment on the performance of combined sampling vs the other approaches we have used for the diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-component",
   "metadata": {},
   "source": [
    "#### As you can see performance of combined sampling using SMOTEENN method we get higher score whereas we get comparatively low performance score when we use regular logisitic regression on model. Score using SMOTEEEN is 78% vs 34% for logisitic regression only. So we can better performance when we use combined sampling method vs just logistic regression approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "editorial-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "sc= StandardScaler()\n",
    "\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "y_predicted = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "closed-religion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3489583333333333"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adequate-juvenile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       100\n",
      "           1       0.76      0.59      0.67        54\n",
      "\n",
      "    accuracy                           0.79       154\n",
      "   macro avg       0.78      0.75      0.76       154\n",
      "weighted avg       0.79      0.79      0.79       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-coral",
   "metadata": {},
   "source": [
    "### 3.\tWhat is outlier detection? Why is it useful? What methods can you use for outlier detection?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "impossible-behavior",
   "metadata": {},
   "source": [
    "Outliers are extreme values that deviate from other observations on data, they may indicate a variability in a measurement, experimental errors or a novelty. Outlier detection is a process of identifying these extreme values from dataset. \n",
    "\n",
    "It is important to detect outliers because outliers may indicate bad data and they affect the mean and median which in turn affects the error in datatset. when you plot the data with outliers then you might get big deviations because of them. For specific algorithm like clustering, outliers can cause distortion of clusters.\n",
    "\n",
    "There are methods to detect outliers but Standard deviation method and Interquartile range method are most common among them. If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers. The distribution of the data in this case is approximately normal. About 68% of the data values lie within one standard deviation of the mean and about 95% are within two standard deviations, and about 99.7% lie within three standard deviations. Therefore, if you have any data point that is more than 3 times the standard deviation, then those points are very likely to be anomalous or outliers.\n",
    "\n",
    "Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution. In this case we can use interquartile range (IQR) method (visually boxplot) to detect outliers. The IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot. The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile.\n",
    "\n",
    "Other method includes DBscan clustering, Isolation forest. In DBscan clustering, it clusters data into groups.It is also used as a density-based anomaly detection method with either single or multi-dimensional data. Other clustering algorithms such as k-means and hierarchal clustering can also be used to detect outliers. Isolation forest is method for unsupervised learning to detect outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-delight",
   "metadata": {},
   "source": [
    "### 4.\tPerform a linear SVM to predict credit approval (last column) using this dataset: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Australian+Credit+Approval%29 . Make sure you look at the accompanying document that describes the data in the dat file. You will need to either convert this data to another file type or import the dat file to python. You can use this code, but otherwise you follow standard practices we have already used many times: \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(kernel='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "confident-scotland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.08</td>\n",
       "      <td>11.460</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21.67</td>\n",
       "      <td>11.500</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>8.170</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1</td>\n",
       "      <td>31.57</td>\n",
       "      <td>10.500</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.415</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>18.83</td>\n",
       "      <td>9.540</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>27.42</td>\n",
       "      <td>14.500</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>3.085</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A1     A2      A3  A4  A5  A6     A7  A8  A9  A10  A11  A12  A13   A14  \\\n",
       "0     1  22.08  11.460   2   4   4  1.585   0   0    0    1    2  100  1213   \n",
       "1     0  22.67   7.000   2   8   4  0.165   0   0    0    0    2  160     1   \n",
       "2     0  29.58   1.750   1   4   4  1.250   0   0    0    1    2  280     1   \n",
       "3     0  21.67  11.500   1   5   3  0.000   1   1   11    1    2    0     1   \n",
       "4     1  20.17   8.170   2   6   4  1.960   1   1   14    0    2   60   159   \n",
       "..   ..    ...     ...  ..  ..  ..    ...  ..  ..  ...  ...  ...  ...   ...   \n",
       "685   1  31.57  10.500   2  14   4  6.500   1   0    0    0    2    0     1   \n",
       "686   1  20.67   0.415   2   8   4  0.125   0   0    0    0    2    0    45   \n",
       "687   0  18.83   9.540   2   6   4  0.085   1   0    0    0    2  100     1   \n",
       "688   0  27.42  14.500   2  14   8  3.085   1   1    1    0    2  120    12   \n",
       "689   1  41.00   0.040   2  10   4  0.040   0   1    1    0    1  560     1   \n",
       "\n",
       "     A15  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      1  \n",
       "..   ...  \n",
       "685    1  \n",
       "686    0  \n",
       "687    1  \n",
       "688    1  \n",
       "689    1  \n",
       "\n",
       "[690 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aus_df = pd.read_csv(\"australian.csv\")\n",
    "aus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "quick-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = aus_df.drop('A15',axis=1)\n",
    "y = aus_df['A15']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45,stratify=y)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "svm_model = clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "double-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59 18]\n",
      " [ 7 54]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "final-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8188405797101449\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "crude-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83        77\n",
      "           1       0.75      0.89      0.81        61\n",
      "\n",
      "    accuracy                           0.82       138\n",
      "   macro avg       0.82      0.83      0.82       138\n",
      "weighted avg       0.83      0.82      0.82       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-texture",
   "metadata": {},
   "source": [
    "### 5.\tHow did the SVM model perform? Use a classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-fitness",
   "metadata": {},
   "source": [
    " SVM model perfomance was decent as you can see from below classification report. As you can see precision (accuracy of positive prediction) was 89 % for class 0 and 75 % for class 1. whereas recall (Fraction of positives that were correctly indentified) was higher on class 1 (89%) vs 77% for class 0. F1 score (what percent of positive predictions were correct) was close in both cases averaged about 82%. Support shows us total number of samples (y_test, which is 20% of total number of columns so 20% of 690 which is equal to 138) and in this report you can see that 77 out of 138 were class 0 and 61 out of 138 were class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "starting-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83        77\n",
      "           1       0.75      0.89      0.81        61\n",
      "\n",
      "    accuracy                           0.82       138\n",
      "   macro avg       0.82      0.83      0.82       138\n",
      "weighted avg       0.83      0.82      0.82       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-consumption",
   "metadata": {},
   "source": [
    "### 6.\tWhat kinds of jobs in data are you most interested in? Do some research on what is out there. Write about your thoughts in under 400 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-impression",
   "metadata": {},
   "source": [
    "There are so many types of roles in data science. Some of them are Data Scientist, Data Engineer, Data Analyst, Machine learning engineer, Business analyst and so on. Machine learning engineering interests me the most followed by data analyst and data engineer. \n",
    "\n",
    "Machine learning engineers make models to predict future outcomes. ML engineers are data- and metric-driven and have a strong foundation in mathematics and statistics. Machine learning engineers have to design and implement machine learning applications/algorithms such as clustering, anomaly detection, classification, or prediction to address business challenges. ML engineers build data pipelines, benchmark infrastructure, and do A/B testing.They work collaboratively with product and development teams to improve data quality via tooling, optimization, and testing. ML engineers have to monitor the performance and ensure the reliability of machine learning systems in the organization.\n",
    "\n",
    "Data analysts are involved in data munging and data visualization. If there are requests from stakeholders, data analysts have to query databases. They are in charge of data that is scraped, assuring the quality and managing it. They have to interpret data and effectively communicate the findings. Data analysts are expected to know R, Python, HTML, SQL, C++, and Javascript. They need to be more than a little familiar with data retrieval and storing systems, data visualization and data warehousing using ETL tools, Hadoop-based analytics, and business intelligence concepts.\n",
    "\n",
    "Data engineers ensure that an organization’s big data ecosystem is running without glitches for data scientists to carry out the analysis. Data infrastructure engineers develop, construct, test, and maintain highly scalable data management systems. Unlike data scientists who seek an exploratory and iterative path to arrive at a solution, data engineers look for the linear path. Data engineers will improve existing systems by integrating newer data management technologies. Data engineers collect and store data, do real-time or batch processing, and serve it for analysis to data scientists via an API. They log and handle errors, identify when to scale up, ensure seamless integration, and “build human-fault-tolerant pipelines.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-answer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
