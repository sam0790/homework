{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "irish-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, mean_squared_error as MSE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-request",
   "metadata": {},
   "source": [
    "### 1. Take one of the supervised learning models you have built recently and apply at least three dimensionality reduction techniques to it (separately). Be sure to create a short summary of each technique you use. Indicate how each changed the model performance. \n",
    "\n",
    "Reference:\n",
    "https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decreased-exclusive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.08</td>\n",
       "      <td>11.460</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21.67</td>\n",
       "      <td>11.500</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>8.170</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1</td>\n",
       "      <td>31.57</td>\n",
       "      <td>10.500</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.415</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>18.83</td>\n",
       "      <td>9.540</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>27.42</td>\n",
       "      <td>14.500</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>3.085</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A1     A2      A3  A4  A5  A6     A7  A8  A9  A10  A11  A12  A13   A14  \\\n",
       "0     1  22.08  11.460   2   4   4  1.585   0   0    0    1    2  100  1213   \n",
       "1     0  22.67   7.000   2   8   4  0.165   0   0    0    0    2  160     1   \n",
       "2     0  29.58   1.750   1   4   4  1.250   0   0    0    1    2  280     1   \n",
       "3     0  21.67  11.500   1   5   3  0.000   1   1   11    1    2    0     1   \n",
       "4     1  20.17   8.170   2   6   4  1.960   1   1   14    0    2   60   159   \n",
       "..   ..    ...     ...  ..  ..  ..    ...  ..  ..  ...  ...  ...  ...   ...   \n",
       "685   1  31.57  10.500   2  14   4  6.500   1   0    0    0    2    0     1   \n",
       "686   1  20.67   0.415   2   8   4  0.125   0   0    0    0    2    0    45   \n",
       "687   0  18.83   9.540   2   6   4  0.085   1   0    0    0    2  100     1   \n",
       "688   0  27.42  14.500   2  14   8  3.085   1   1    1    0    2  120    12   \n",
       "689   1  41.00   0.040   2  10   4  0.040   0   1    1    0    1  560     1   \n",
       "\n",
       "     A15  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      1  \n",
       "..   ...  \n",
       "685    1  \n",
       "686    0  \n",
       "687    1  \n",
       "688    1  \n",
       "689    1  \n",
       "\n",
       "[690 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../australian.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominant-music",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "christian-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing outliers with nulls in all the feature columns\n",
    "\n",
    "for x in ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14']:\n",
    "    q75,q25 = np.percentile(df.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    "\n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    "\n",
    "    df.loc[df[x] < min,x] = np.nan\n",
    "    df.loc[df[x] > max,x] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-cleaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1       0\n",
       "A2      18\n",
       "A3      17\n",
       "A4     165\n",
       "A5       0\n",
       "A6     215\n",
       "A7      63\n",
       "A8       0\n",
       "A9       0\n",
       "A10     79\n",
       "A11      0\n",
       "A12     65\n",
       "A13     13\n",
       "A14    113\n",
       "A15      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "charged-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping all null values from Dataframe\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mature-airplane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.42</td>\n",
       "      <td>6.500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.58</td>\n",
       "      <td>2.710</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.92</td>\n",
       "      <td>9.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>592.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>41.17</td>\n",
       "      <td>1.335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.25</td>\n",
       "      <td>1.500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37.75</td>\n",
       "      <td>5.500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.415</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.83</td>\n",
       "      <td>9.540</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A1     A2     A3   A4    A5   A6     A7   A8   A9  A10  A11  A12    A13  \\\n",
       "1    0.0  22.67  7.000  2.0   8.0  4.0  0.165  0.0  0.0  0.0  0.0  2.0  160.0   \n",
       "6    1.0  17.42  6.500  2.0   3.0  4.0  0.125  0.0  0.0  0.0  0.0  2.0   60.0   \n",
       "14   1.0  58.58  2.710  2.0   8.0  4.0  2.415  0.0  0.0  0.0  1.0  2.0  320.0   \n",
       "17   0.0  18.92  9.000  2.0   6.0  4.0  0.750  1.0  1.0  2.0  0.0  2.0   88.0   \n",
       "22   1.0  41.17  1.335  2.0   2.0  4.0  0.165  0.0  0.0  0.0  0.0  2.0  168.0   \n",
       "..   ...    ...    ...  ...   ...  ...    ...  ...  ...  ...  ...  ...    ...   \n",
       "664  1.0  20.00  0.000  2.0   2.0  4.0  0.500  0.0  0.0  0.0  0.0  2.0  144.0   \n",
       "668  0.0  32.25  1.500  2.0   8.0  4.0  0.250  0.0  0.0  0.0  1.0  2.0  372.0   \n",
       "670  0.0  37.75  5.500  2.0  11.0  4.0  0.125  1.0  0.0  0.0  1.0  2.0  228.0   \n",
       "686  1.0  20.67  0.415  2.0   8.0  4.0  0.125  0.0  0.0  0.0  0.0  2.0    0.0   \n",
       "687  0.0  18.83  9.540  2.0   6.0  4.0  0.085  1.0  0.0  0.0  0.0  2.0  100.0   \n",
       "\n",
       "       A14  A15  \n",
       "1      1.0    0  \n",
       "6    101.0    0  \n",
       "14     1.0    0  \n",
       "17   592.0    1  \n",
       "22     1.0    0  \n",
       "..     ...  ...  \n",
       "664    1.0    0  \n",
       "668  123.0    0  \n",
       "670    1.0    1  \n",
       "686   45.0    0  \n",
       "687    1.0    1  \n",
       "\n",
       "[225 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe after removing null values\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-olive",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automatic-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('A15',axis=1)\n",
    "y = df['A15']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minimal-church",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "organizational-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data score:  0.85\n"
     ]
    }
   ],
   "source": [
    "print('Training data score: ',model.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "paperback-burner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data score:  0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "print('Test data score: ',model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fourth-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-moses",
   "metadata": {},
   "source": [
    "### PCA method \n",
    "\n",
    "#### Principal Component Analysis, or PCA, might be the most popular technique for dimensionality reduction with dense data. On this dataset we do not see great permormance result because this dataset is not very dense and complex. It is just simple dataset with only 15 columns and 225 rows after preprocessing done. When we ran this dataset using logisitic regression we were getting accuracy score of 0.91 and when we ran using PCA method it was giving us around 0.77 for test data. As we reduces features for dataset our accuracy goes down because in this dataset features are not entirely dependent. So we get less accuracy if we include less features. First we set variable named steps with PCA and logistic regression method and use this variable in model. Then we declared CV variable with 10 splits and chekced score using model and cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coordinated-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.706 (0.118)\n",
      "Test Accuracy: 0.773 (0.169)\n"
     ]
    }
   ],
   "source": [
    "steps = [('pca', PCA(n_components=7)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "n_scores_tr = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "n_scores_te = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Training Accuracy: %.3f (%.3f)' % (mean(n_scores_tr), std(n_scores_tr)))\n",
    "print('Test Accuracy: %.3f (%.3f)' % (mean(n_scores_te), std(n_scores_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-delhi",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition Method\n",
    "\n",
    "#### Singular Value Decomposition, or SVD, is one of the most popular techniques for dimensionality reduction for sparse data (data with many zero values). As we applied this method with 7 features insted of 14 we got around 0.77 accuracy score for test data which is similar to PCA method. The reason for this is same as we decrease the number of features our accuracy drops as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "published-agriculture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.709 (0.088)\n",
      "Test Accuracy: 0.775 (0.226)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "steps = [('svd', TruncatedSVD(n_components=7)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores_tr = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "n_scores_te = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Training Accuracy: %.3f (%.3f)' % (mean(n_scores_tr), std(n_scores_tr)))\n",
    "print('Test Accuracy: %.3f (%.3f)' % (mean(n_scores_te), std(n_scores_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-latino",
   "metadata": {},
   "source": [
    "### Isomap Embedding Method\n",
    "\n",
    "#### Isomap Embedding, or Isomap, creates an embedding of the dataset and attempts to preserve the relationships in the dataset. Using this method we got accuracy score of 0.66 using 7 features. This method gave us the worse performance out of other methods we used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "compact-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.624 (0.079)\n",
      "Test Accuracy: 0.663 (0.178)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "steps = [('iso', Isomap(n_components=7)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores_tr = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "n_scores_te = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Training Accuracy: %.3f (%.3f)' % (mean(n_scores_tr), std(n_scores_tr)))\n",
    "print('Test Accuracy: %.3f (%.3f)' % (mean(n_scores_te), std(n_scores_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-longer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "double-hazard",
   "metadata": {},
   "source": [
    "### 2. Write a function that will indicate if an inputted IPv4 address is accurate or not. IP addresses are valid if they have 4 values between 0 and 255 (inclusive), punctuated by periods.\n",
    "\n",
    "Input 1:\n",
    "2.33.245.5\n",
    "Output 1:\n",
    "True\n",
    "\n",
    "Input 2:\n",
    "12.345.67.89\n",
    "Output 2:\n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "architectural-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_not(x):\n",
    "    \n",
    "    x=str(x)\n",
    "    \n",
    "    y = x.split(\".\")\n",
    "    \n",
    "    if len(y) == 4:\n",
    "        for i in y:\n",
    "            if int(i)>255 or int(i)<0:\n",
    "                return False\n",
    "            \n",
    "        return True\n",
    "          \n",
    "    else:\n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "general-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = \"2.33.453.5\"\n",
    "\n",
    "#print(a)\n",
    "print(ip_not('2.33.245.5'))\n",
    "\n",
    "print(ip_not('12.345.67.89'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
